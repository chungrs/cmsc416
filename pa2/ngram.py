"""
Programming Assignment 2 - N-Gram Language Model

Roy Chung
20210303
CMSC 416-001

ngram.py is a program that learns an N-gram language model from an arbitrary number of plain text files
supplied by the user, whose filenames are passed in as command line arguments. To run this program,
place plain text files from which the program will learn the n-gram language model in the same directory
as ngram.py. To run it, type the following into the command line:

ngram.py n m filename1.txt filename2.txt [...]

n denotes the number of previous words taken into consideration when generating the next word. Enter a positive
integer for this value. The greater n is, the more comprehensible the generated sentences will be.

m is the number of sentences you want generated by the program. Enter a positive integer.

You must supply at least one plain text file for this program to work. Valid plain text files include those
available on the Project Gutenberg website (http://www.gutenberg.org). List the filenames as they appear downloaded
into the same directory in which ngram.py is located.

Sample run using  "Crime and Punishment," "War and Peace," and "Anna Karenina" from Project Gutenberg:

%python ngram.py 5 10 pg2554.txt pg2600.txt pg1399.txt

1.	Writing out on note paper in his minute hand all that he owed , he added up the amount and found that his debts
    amounted to seventeen thousand and some odd hundreds , which he left out for the sake of .

2.	You seem to find something wrong in my speaking like that about women ?

3.	After a while the moving mass became agitated , someone rode past on a white horse followed by his suite , and the
    boyish face of rostóv , breathless with excitement and hope , was the first to gallop into the village street .

4.	He sat motionless , looking at the changing shadows before him .

5.	Crime and punishment part i chapter i on an exceptionally hot evening early in july a young man came out of the
    governor ’ s , as you ordered , ” answered the girl , smiling still more brightly .

6.	He started a journal - - “ the epoch , ” which within a few months was also prohibited .

7.	You can ’ t imagine how we felt !

8.	Though pulcheria alexandrovna felt that the young man was very glad to die .

9.	Though lebeziatnikov was so good - natured , pale , young face to look back .

10.	You must have pity , sir , on the children .


This task was accomplished by converting the text of the files into tokens (words, numbers, punctuation marks -
everything except for spaces), adding <start> and <end> tokens to the beginnings and ends, respectively, of sentences,
and organizing those tokens into ngrams. The frequency of ngrams and words are calculated by the program, and the next
word is generated semi-randomly with consideration to its relative probability of being the next word given the context.
Words with 0 probability given the plain text files should not occur, while those with higher probability of occurring
are more likely to be generated.
"""

from sys import argv
import re
import random
from random import random

def print_project_description():
    print("This program generates random sentences based on an Ngram model.")


# Reads arguments from the command line and processes the information.
# Reads text files specified by command line arguments and returns them as tokens and ngrams
def import_data():
    tokens, ngrams = [], []
    # If there are less than 4 arguments, return an error and exit the program
    if len(argv) < 4:
        print("Invalid command line argument. Please enter ngram.py n m input_file1.txt input_file2.txt....\n"
              "n: n-gram\n"
              "m: number of sentences to generate\n"
              "input_fileX.txt: at least 1 plain text file from which the program will learn an N-gram language model.")
        exit()

    n = int(argv[1])
    m = int(argv[2])

    # If n is less than 1, return an error and exit the program
    if n < 1:
        print("n should be greater than 0.")
        exit()

    # If n is less than 1, return an error and exit the program
    if m < 1:
        print("m should be greater than 0.")
        exit()

    # Output command line arguments
    print("Command line settings: ngram.py " + ' '.join([str(elem) for elem in argv[1:]]))

    for files in argv[3:]:
        with open(files, errors='ignore', encoding='utf-8-sig') as file:
            sentence_tokens = get_sentence_tokens(file.read())  # tokenize the text to sentences
            for sentence in sentence_tokens:
                # extract words from sentence as candidate tokens
                candidate_tokens = get_word_tokens(sentence, n)

                # Discard candidate tokens if sentence length is smaller than n-2
                # (n-2 accounts for (n-1)*<start> and <end> tokens)
                if len(candidate_tokens) - n - 2 >= n:
                    ngrams += generate_ngrams(n, candidate_tokens)
                    for token in candidate_tokens:
                        tokens.append(token)

    return n, m, tokens, ngrams


# Returns the contents of a list as a string with elements delineated by spaces
def list_to_string(lst):
    string = " "
    return string.join(lst)


# Function for converting pre-processed sentences into tokens
def get_word_tokens(sentence, n):
    # Add spaces, which serve as delimiters, around punctuation marks, so punctuation marks become tokens
    tokens = re.findall(r"[\w]+|[^\s\w]", sentence)
    # Insert <start> to the beginning of the list n-1 times
    for i in range(n-1):
        tokens.insert(0, "<start>")
    # Append <end> tag to the end of the list
    tokens.append("<end>")

    return tokens


# Function for extracting sentence tokens from text
def get_sentence_tokens(text):
    text = text.lower()  # Convert text to lowercase
    text = text.replace("\n", " ")
    tokens = re.split(r"(?<=[\.\!\?])\s*", text)  # split sentence where ., !, and ? are found
    tokens = [i for i in tokens if i]  # remove empty strings from list
    return tokens


# Receives tokens and integer n and creates ngrams with zip function
def generate_ngrams(n, tokens):
    ngrams = zip(*[tokens[i:] for i in range(n)])
    return [" ".join(ngram) for ngram in ngrams]


# Iterates through list of ngrams and returns the relative frequency distribution table for words in relation to
# words/phrases preceding to them.
def get_num_freq_dist(ngrams, n):
    freq_dist = {}

    for i in range(0, len(ngrams)):
        word = str(ngrams[i].split()[n - 1])
        context = list_to_string(ngrams[i].split()[:(n - 1)])
        # Increment frequency of context if context already exists in freq_dist
        if context in freq_dist:
            if word in freq_dist[context]:
                freq_dist[context][word] += 1
            else:
                freq_dist[context][word] = {}
                freq_dist[context][word] = 1
        # If context does not yet exist in freq_dist, initialize it and set to 1
        else:
            freq_dist[context] = {}
            freq_dist[context][word] = {}
            freq_dist[context][word] = 1

    return freq_dist


# Iterates through list of ngrams and returns the frequency distribution table for words/phrases preceding candidate
# words.
def get_dem_freq_dist(ngrams, n):
    freq_dist = {}

    for i in range(0, len(ngrams)):
        context = list_to_string(ngrams[i].split()[:(n - 1)])
        # Increment frequency of context if context already exists in freq_dist
        if context in freq_dist:
            freq_dist[context] += 1
        # If context does not yet exist in freq_dist, initialize it and set to 1
        else:
            freq_dist[context] = {}
            freq_dist[context] = 1
    return freq_dist


# Sentence generator specifically for unigrams, since Markov's Assumption cannot be applied
def generate_unigram_sentence(word_total, word_freq):
    sentence = ""
    ngram_array = [" "]

    # Runs until <end> marker is reached
    while "<end>" not in ngram_array:
        total = 0
        for word in word_freq:
            prob = word_freq[word] / word_total
            total += prob
            if random() < total:
                sentence = sentence + ngram_array.pop(0) + " "
                ngram_array.append(word)

                if "<end>" in ngram_array:
                    sentence += " ".join(ngram_array)
                break

    sentence = sentence.replace(" <end>", ".")

    # remove non-alphanumeric leading characters
    while not sentence[0].isalpha():
        sentence = sentence[1:]

    return sentence.capitalize()


# Sentences are generated by calculating the probability of the next word based on the preceding words in accordance to
# Markov's Assumption. A random number between 0 and 1 is generated, which determines the next word in the sentence.
# This process continues until an <end> marker is reached.
def generate_sentence(n, num_freq_dist, dem_freq_dist):
    sentence = ""
    ngram_list = []

    for n in range(0, (n - 1)):
        ngram_list.append("<start>")

    # Runs until <end> marker is reached
    while "<end>" not in ngram_list:
        context = " ".join(ngram_list)
        word_freq = num_freq_dist[context]
        total = 0
        for word in word_freq:
            prob = word_freq[word] / dem_freq_dist[context]
            total += prob
            if random() < total:
                sentence = sentence + ngram_list.pop(0) + " "
                ngram_list.append(word)

                if "<end>" in ngram_list:
                    sentence += " ".join(ngram_list)
                break

    sentence = sentence.replace("<start>", " ")
    sentence = sentence.replace(" <end>", "")

    # remove non-alphanumeric leading characters
    while not sentence[0].isalpha():
        sentence = sentence[1:]

    return sentence.capitalize()


def main():
    print_project_description()

    # receive n, m, tokens, and ngrams from import_data()
    n, m, tokens, ngrams = import_data()

    # print(tokens)
    # print(ngrams)

    # ==================== UNIGRAM ====================
    if n == 1:
        word_freq = {}
        for each in ngrams:
            if each in word_freq:
                word_freq[each] += 1
                continue
            word_freq[each] = 1

        word_total = sum(word_freq.values()) - word_freq["<end>"]

        for m in range(0, m):
            sentence = generate_unigram_sentence(word_total, word_freq)
            print(str(m+1) + ".\t" + sentence)
    # ====================  NGRAM  ====================
    elif n > 1:
        num_freq_dist = get_num_freq_dist(ngrams, n)
        dem_freq_dist = get_dem_freq_dist(ngrams, n)
        for m in range(0, m):
            sentence = generate_sentence(n, num_freq_dist, dem_freq_dist)
            print(str(m+1) + ".\t" + sentence)


if __name__ == "__main__":
    main()
